{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from samay.model import LPTMModel\n",
    "\n",
    "config = {\n",
    "    \"task_name\": \"forecasting\",\n",
    "    \"forecast_horizon\": 192,\n",
    "    \"head_dropout\": 0,\n",
    "    \"weight_decay\": 0,\n",
    "    \"max_patch\": 16,\n",
    "    \"freeze_encoder\": True,  # Freeze the patch embedding layer\n",
    "    \"freeze_embedder\": True,  # Freeze the transformer encoder\n",
    "    \"freeze_head\": False,  # The linear forecasting head must be trained\n",
    "    \"freeze_segment\": True,  # Freeze the segmention module\n",
    "}\n",
    "model = LPTMModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from samay.dataset import LPTMDataset\n",
    "\n",
    "train_dataset = LPTMDataset(\n",
    "    name=\"ett\",\n",
    "    datetime_col=\"date\",\n",
    "    path=\"../data/data/ETTh1.csv\",\n",
    "    mode=\"train\",\n",
    "    horizon=192,\n",
    ")\n",
    "\n",
    "finetuned_model = model.finetune(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.samay.model import LPTMModel\n",
    "\n",
    "model = model.quantize(quant_type = \"int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = LPTMDataset(\n",
    "    name=\"ett\",\n",
    "    datetime_col=\"date\",\n",
    "    path=\"../data/data/ETTh1.csv\",\n",
    "    mode=\"train\",\n",
    "    horizon=192,\n",
    ")\n",
    "metrics, trues, preds, histories = model.evaluate(val_dataset, task_name=\"forecasting\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
